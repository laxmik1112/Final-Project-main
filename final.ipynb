{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8c004fc",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0e2eabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anike\\miniconda3\\envs\\K\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "from string import Template\n",
    "from timeit import default_timer as timer\n",
    "from dotenv import load_dotenv\n",
    "from time import sleep\n",
    "from groq import Groq\n",
    "from transformers import pipeline\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "from abc import ABC, abstractmethod\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee1ce929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dddc974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize clients with error handling\n",
    "try:\n",
    "    groq_client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "    if not os.getenv(\"GROQ_API_KEY\"):\n",
    "        raise ValueError(\"GROQ_API_KEY not found in environment variables\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to initialize Groq client: {e}\")\n",
    "    groq_client = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2c03e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Initialize confidence scorer with error handling\n",
    "try:\n",
    "    classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to initialize classifier: {e}\")\n",
    "    classifier = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580587f1",
   "metadata": {},
   "source": [
    "# GRAPH DATABASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b64a3c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Entity:\n",
    "    \"\"\"Data class for entities\"\"\"\n",
    "    id: str\n",
    "    label: str\n",
    "    properties: Dict[str, Any]\n",
    "\n",
    "@dataclass\n",
    "class Relationship:\n",
    "    \"\"\"Data class for relationships\"\"\"\n",
    "    source: str\n",
    "    target: str\n",
    "    type: str\n",
    "    properties: Dict[str, Any]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5dab52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphDatabaseConnector(ABC):\n",
    "    \"\"\"Abstract base class for graph database connectors\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def connect(self) -> bool:\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def close(self):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def create_entity(self, entity: Entity) -> bool:\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def create_relationship(self, relationship: Relationship) -> bool:\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def query(self, query: str, parameters: Dict[str, Any] = None) -> List[Dict[str, Any]]:\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def clear_database(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "778c5ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SQLiteGraphConnector(GraphDatabaseConnector):\n",
    "    \"\"\"SQLite-based graph database connector for lightweight usage\"\"\"\n",
    "    \n",
    "    def __init__(self, db_path: str = \"graph_data.db\"):\n",
    "        self.db_path = db_path\n",
    "        self.connection = None\n",
    "    \n",
    "    def connect(self) -> bool:\n",
    "        \"\"\"Connect to SQLite database\"\"\"\n",
    "        try:\n",
    "            self.connection = sqlite3.connect(self.db_path)\n",
    "            self.connection.row_factory = sqlite3.Row\n",
    "            self._create_tables()\n",
    "            logger.info(f\"Connected to SQLite database: {self.db_path}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to connect to SQLite: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def _create_tables(self):\n",
    "        \"\"\"Create tables for entities and relationships\"\"\"\n",
    "        cursor = self.connection.cursor()\n",
    "        \n",
    "        # Entities table\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS entities (\n",
    "                id TEXT PRIMARY KEY,\n",
    "                label TEXT NOT NULL,\n",
    "                properties TEXT,\n",
    "                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "            )\n",
    "        \"\"\")\n",
    "        \n",
    "        # Relationships table\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS relationships (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                source TEXT NOT NULL,\n",
    "                target TEXT NOT NULL,\n",
    "                type TEXT NOT NULL,\n",
    "                properties TEXT,\n",
    "                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                FOREIGN KEY (source) REFERENCES entities (id),\n",
    "                FOREIGN KEY (target) REFERENCES entities (id)\n",
    "            )\n",
    "        \"\"\")\n",
    "        \n",
    "        # Create indexes for better performance\n",
    "        cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_entities_label ON entities(label)\")\n",
    "        cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_relationships_source ON relationships(source)\")\n",
    "        cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_relationships_target ON relationships(target)\")\n",
    "        cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_relationships_type ON relationships(type)\")\n",
    "        \n",
    "        self.connection.commit()\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close database connection\"\"\"\n",
    "        if self.connection:\n",
    "            self.connection.close()\n",
    "            logger.info(\"SQLite connection closed\")\n",
    "    \n",
    "    def create_entity(self, entity: Entity) -> bool:\n",
    "        \"\"\"Create entity in SQLite\"\"\"\n",
    "        try:\n",
    "            cursor = self.connection.cursor()\n",
    "            cursor.execute(\n",
    "                \"INSERT OR REPLACE INTO entities (id, label, properties) VALUES (?, ?, ?)\",\n",
    "                (entity.id, entity.label, json.dumps(entity.properties))\n",
    "            )\n",
    "            self.connection.commit()\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to create entity {entity.id}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def create_relationship(self, relationship: Relationship) -> bool:\n",
    "        \"\"\"Create relationship in SQLite\"\"\"\n",
    "        try:\n",
    "            cursor = self.connection.cursor()\n",
    "            # Check if relationship already exists to avoid duplicates\n",
    "            cursor.execute(\n",
    "                \"SELECT id FROM relationships WHERE source = ? AND target = ? AND type = ?\",\n",
    "                (relationship.source, relationship.target, relationship.type)\n",
    "            )\n",
    "            if cursor.fetchone():\n",
    "                logger.debug(f\"Relationship already exists: {relationship.source}->{relationship.target}\")\n",
    "                return True\n",
    "            \n",
    "            cursor.execute(\n",
    "                \"INSERT INTO relationships (source, target, type, properties) VALUES (?, ?, ?, ?)\",\n",
    "                (relationship.source, relationship.target, relationship.type, \n",
    "                 json.dumps(relationship.properties))\n",
    "            )\n",
    "            self.connection.commit()\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to create relationship {relationship.source}->{relationship.target}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def query(self, query: str, parameters: Dict[str, Any] = None) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Execute SQL query\"\"\"\n",
    "        try:\n",
    "            cursor = self.connection.cursor()\n",
    "            if parameters:\n",
    "                # Handle both dict and tuple parameters\n",
    "                if isinstance(parameters, dict):\n",
    "                    # Convert named parameters to positional for SQLite\n",
    "                    cursor.execute(query, tuple(parameters.values()) if parameters else ())\n",
    "                else:\n",
    "                    cursor.execute(query, parameters)\n",
    "            else:\n",
    "                cursor.execute(query)\n",
    "            rows = cursor.fetchall()\n",
    "            return [dict(row) for row in rows]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Query failed: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def clear_database(self):\n",
    "        \"\"\"Clear all data from database\"\"\"\n",
    "        try:\n",
    "            cursor = self.connection.cursor()\n",
    "            cursor.execute(\"DELETE FROM relationships\")\n",
    "            cursor.execute(\"DELETE FROM entities\")\n",
    "            self.connection.commit()\n",
    "            logger.info(\"Database cleared\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to clear database: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2970fb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_graph_data(data: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Validate the structure of extracted graph data.\"\"\"\n",
    "    required_keys = ['entities', 'relationships']\n",
    "    \n",
    "    if not all(key in data for key in required_keys):\n",
    "        logger.error(f\"Missing required keys. Expected: {required_keys}, Got: {list(data.keys())}\")\n",
    "        return False\n",
    "    \n",
    "    if not isinstance(data['entities'], list):\n",
    "        logger.error(\"Entities should be a list\")\n",
    "        return False\n",
    "    \n",
    "    if not isinstance(data['relationships'], list):\n",
    "        logger.error(\"Relationships should be a list\")\n",
    "        return False\n",
    "    \n",
    "    # Validate entity structure\n",
    "    for entity in data['entities']:\n",
    "        if not isinstance(entity, dict) or 'id' not in entity or 'label' not in entity:\n",
    "            logger.error(f\"Invalid entity structure: {entity}\")\n",
    "            return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90091bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_in_graph_database(data: Dict[str, Any], db_connector: SQLiteGraphConnector) -> bool:\n",
    "    \"\"\"Store extracted data in graph database\"\"\"\n",
    "    if not validate_graph_data(data):\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # Store entities\n",
    "        for entity_data in data['entities']:\n",
    "            entity_id = str(entity_data['id'])\n",
    "            label = entity_data['label']\n",
    "            properties = {k: v for k, v in entity_data.items() if k not in ['id', 'label']}\n",
    "            \n",
    "            entity = Entity(id=entity_id, label=label, properties=properties)\n",
    "            if not db_connector.create_entity(entity):\n",
    "                logger.error(f\"Failed to store entity: {entity_id}\")\n",
    "                return False\n",
    "        \n",
    "        # Store relationships\n",
    "        for rel in data['relationships']:\n",
    "            try:\n",
    "                if isinstance(rel, str) and \"|\" in rel:\n",
    "                    parts = rel.split(\"|\")\n",
    "                    if len(parts) >= 3:\n",
    "                        source, rel_type, target = parts[0], parts[1], parts[2]\n",
    "                        properties = {\"confidence\": score_confidence(f\"{source} {rel_type} {target}\", rel_type)}\n",
    "                    else:\n",
    "                        logger.warning(f\"Invalid relationship format: {rel}\")\n",
    "                        continue\n",
    "                elif isinstance(rel, dict):\n",
    "                    source = str(rel.get('source', ''))\n",
    "                    target = str(rel.get('target', ''))\n",
    "                    rel_type = rel.get('type', 'RELATED_TO')\n",
    "                    properties = {k: v for k, v in rel.items() if k not in ['source', 'target', 'type']}\n",
    "                else:\n",
    "                    logger.warning(f\"Unsupported relationship format: {rel}\")\n",
    "                    continue\n",
    "                \n",
    "                relationship = Relationship(\n",
    "                    source=source, \n",
    "                    target=target, \n",
    "                    type=rel_type, \n",
    "                    properties=properties\n",
    "                )\n",
    "                \n",
    "                if not db_connector.create_relationship(relationship):\n",
    "                    logger.error(f\"Failed to store relationship: {source}->{target}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing relationship {rel}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error storing data in graph database: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8b85a4",
   "metadata": {},
   "source": [
    "# GRAPH ANALYTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "213b16cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAnalytics:\n",
    "    \"\"\"Analytics and querying utilities for graph data\"\"\"\n",
    "    \n",
    "    def __init__(self, db_connector: SQLiteGraphConnector):\n",
    "        self.db = db_connector\n",
    "    \n",
    "    def get_entity_counts(self) -> Dict[str, int]:\n",
    "        \"\"\"Get count of entities by label/type\"\"\"\n",
    "        query = \"SELECT label, COUNT(*) as count FROM entities GROUP BY label ORDER BY count DESC\"\n",
    "        results = self.db.query(query)\n",
    "        return {row['label']: row['count'] for row in results}\n",
    "    \n",
    "    def get_relationship_counts(self) -> Dict[str, int]:\n",
    "        \"\"\"Get count of relationships by type\"\"\"\n",
    "        query = \"SELECT type, COUNT(*) as count FROM relationships GROUP BY type ORDER BY count DESC\"\n",
    "        results = self.db.query(query)\n",
    "        return {row['type']: row['count'] for row in results}\n",
    "    \n",
    "    def find_central_entities(self, limit: int = 10) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Find most connected entities\"\"\"\n",
    "        query = \"\"\"\n",
    "        SELECT e.id, e.label, \n",
    "               JSON_EXTRACT(e.properties, '$.name') as name,\n",
    "               (SELECT COUNT(*) FROM relationships WHERE source = e.id OR target = e.id) as connections\n",
    "        FROM entities e\n",
    "        ORDER BY connections DESC\n",
    "        LIMIT ?\n",
    "        \"\"\"\n",
    "        return self.db.query(query, {\"limit\": limit})\n",
    "    \n",
    "    def find_related_entities(self, entity_id: str, relationship_type: str = None) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Find entities related to a given entity\"\"\"\n",
    "        if relationship_type:\n",
    "            query = \"\"\"\n",
    "            SELECT DISTINCT e.id, e.label, JSON_EXTRACT(e.properties, '$.name') as name, r.type\n",
    "            FROM entities e\n",
    "            JOIN relationships r ON (r.source = e.id OR r.target = e.id)\n",
    "            WHERE (r.source = ? OR r.target = ?) AND e.id != ? AND r.type = ?\n",
    "            \"\"\"\n",
    "            parameters = {\"entity_id\": entity_id, \"type\": relationship_type}\n",
    "        else:\n",
    "            query = \"\"\"\n",
    "            SELECT DISTINCT e.id, e.label, JSON_EXTRACT(e.properties, '$.name') as name, r.type\n",
    "            FROM entities e\n",
    "            JOIN relationships r ON (r.source = e.id OR r.target = e.id)\n",
    "            WHERE (r.source = ? OR r.target = ?) AND e.id != ?\n",
    "            \"\"\"\n",
    "            parameters = {\"entity_id\": entity_id}\n",
    "        \n",
    "        return self.db.query(query, parameters)\n",
    "    \n",
    "    def get_entity_neighbors(self, entity_id: str, depth: int = 1) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Get neighboring entities within specified depth\"\"\"\n",
    "        query = \"\"\"\n",
    "        SELECT DISTINCT e.id, e.label, JSON_EXTRACT(e.properties, '$.name') as name\n",
    "        FROM entities e\n",
    "        JOIN relationships r ON (r.source = e.id OR r.target = e.id)\n",
    "        WHERE (r.source = ? OR r.target = ?) AND e.id != ?\n",
    "        \"\"\"\n",
    "        return self.db.query(query, {\"entity_id\": entity_id})\n",
    "    \n",
    "    def find_entities_by_label(self, label: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Find all entities with a specific label\"\"\"\n",
    "        query = \"SELECT id, label, properties FROM entities WHERE label = ?\"\n",
    "        return self.db.query(query, {\"label\": label})\n",
    "    \n",
    "    def get_relationship_details(self, source_id: str = None, target_id: str = None, rel_type: str = None) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Get detailed information about relationships\"\"\"\n",
    "        conditions = []\n",
    "        params = {}\n",
    "        \n",
    "        if source_id:\n",
    "            conditions.append(\"r.source = ?\")\n",
    "            params[\"source\"] = source_id\n",
    "        if target_id:\n",
    "            conditions.append(\"r.target = ?\")\n",
    "            params[\"target\"] = target_id\n",
    "        if rel_type:\n",
    "            conditions.append(\"r.type = ?\")\n",
    "            params[\"type\"] = rel_type\n",
    "        \n",
    "        where_clause = \"WHERE \" + \" AND \".join(conditions) if conditions else \"\"\n",
    "        \n",
    "        query = f\"\"\"\n",
    "        SELECT r.source, r.target, r.type, r.properties,\n",
    "               e1.label as source_label, JSON_EXTRACT(e1.properties, '$.name') as source_name,\n",
    "               e2.label as target_label, JSON_EXTRACT(e2.properties, '$.name') as target_name\n",
    "        FROM relationships r\n",
    "        JOIN entities e1 ON r.source = e1.id\n",
    "        JOIN entities e2 ON r.target = e2.id\n",
    "        {where_clause}\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.db.query(query, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31d3a2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_analytics_demo(analytics: GraphAnalytics):\n",
    "    \"\"\"Demonstrate analytics capabilities\"\"\"\n",
    "    logger.info(\"Running analytics demo...\")\n",
    "    \n",
    "    # Entity counts\n",
    "    entity_counts = analytics.get_entity_counts()\n",
    "    logger.info(f\"Entity counts: {entity_counts}\")\n",
    "    \n",
    "    # Relationship counts\n",
    "    rel_counts = analytics.get_relationship_counts()\n",
    "    logger.info(f\"Relationship counts: {rel_counts}\")\n",
    "    \n",
    "    # Central entities\n",
    "    central_entities = analytics.find_central_entities(5)\n",
    "    logger.info(f\"Most connected entities: {central_entities}\")\n",
    "    \n",
    "    # Find entities by label\n",
    "    projects = analytics.find_entities_by_label(\"Project\")\n",
    "    logger.info(f\"Projects found: {len(projects)}\")\n",
    "    \n",
    "    # Show sample relationships\n",
    "    relationships = analytics.get_relationship_details()[:5]  # First 5 relationships\n",
    "    logger.info(f\"Sample relationships: {relationships}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f442615",
   "metadata": {},
   "source": [
    "# LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0280d447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_llm_query(file_prompt: str, system_msg: str, max_retries: int = 3) -> Optional[str]:\n",
    "    \"\"\"Call the Groq LLM with retry logic and better error handling.\"\"\"\n",
    "    if not groq_client:\n",
    "        logger.error(\"Groq client not initialized\")\n",
    "        return None\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = groq_client.chat.completions.create(\n",
    "                model=\"llama3-70b-8192\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_msg},\n",
    "                    {\"role\": \"user\", \"content\": file_prompt},\n",
    "                ],\n",
    "                temperature=0.5,\n",
    "                max_tokens=2048,\n",
    "            )\n",
    "            content = response.choices[0].message.content\n",
    "            sleep(1)  # Reduced sleep time\n",
    "            return content\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                sleep(2 ** attempt)  # Exponential backoff\n",
    "            else:\n",
    "                logger.error(f\"All attempts failed for LLM query\")\n",
    "                return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c49869",
   "metadata": {},
   "source": [
    "# CONFIDENCE SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a8ab8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_confidence(text: str, label: str) -> float:\n",
    "    \"\"\"Score entity and relationship confidence using zero-shot classification.\"\"\"\n",
    "    if not classifier:\n",
    "        logger.warning(\"Classifier not available, returning default confidence\")\n",
    "        return 3.0  # Default middle confidence\n",
    "    \n",
    "    try:\n",
    "        result = classifier(text, [label], multi_label=False)\n",
    "        return result['scores'][0] * 5  # Convert to 1-5 scale\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error scoring confidence: {e}\")\n",
    "        return 3.0  # Default confidence on error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cde6ab8",
   "metadata": {},
   "source": [
    "# VISUALIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0964ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_graph(data: Dict[str, Any], output_file: Optional[str] = None) -> bool:\n",
    "    \"\"\"Visualize graph with confidence scores and explanations.\"\"\"\n",
    "    if not validate_graph_data(data):\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        G = nx.DiGraph()\n",
    "        labels = {}\n",
    "\n",
    "        # Add nodes\n",
    "        for entity in data['entities']:\n",
    "            entity_id = str(entity['id'])\n",
    "            entity_label = entity.get('label', 'Unknown')\n",
    "            entity_name = entity.get('name', entity_id)\n",
    "            \n",
    "            G.add_node(entity_id, label=entity_label)\n",
    "            labels[entity_id] = f\"{entity_label}\\n{entity_name}\"\n",
    "\n",
    "        # Add edges\n",
    "        edge_confidences = {}\n",
    "        for rel in data['relationships']:\n",
    "            try:\n",
    "                if isinstance(rel, str) and \"|\" in rel:\n",
    "                    parts = rel.split(\"|\")\n",
    "                    if len(parts) >= 3:\n",
    "                        head, relation, tail = parts[0], parts[1], parts[2]\n",
    "                    else:\n",
    "                        logger.warning(f\"Invalid relationship format: {rel}\")\n",
    "                        continue\n",
    "                elif isinstance(rel, dict):\n",
    "                    head = str(rel.get('source', ''))\n",
    "                    tail = str(rel.get('target', ''))\n",
    "                    relation = rel.get('type', 'RELATED_TO')\n",
    "                else:\n",
    "                    logger.warning(f\"Unsupported relationship format: {rel}\")\n",
    "                    continue\n",
    "                \n",
    "                # Only add edge if both nodes exist\n",
    "                if head in [str(e['id']) for e in data['entities']] and tail in [str(e['id']) for e in data['entities']]:\n",
    "                    text = f\"{head} {relation} {tail}\"\n",
    "                    confidence = score_confidence(text, relation)\n",
    "                    G.add_edge(head, tail, label=relation, weight=confidence)\n",
    "                    edge_confidences[(head, tail)] = confidence\n",
    "                else:\n",
    "                    logger.warning(f\"Relationship references non-existent entities: {head} -> {tail}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing relationship {rel}: {e}\")\n",
    "                continue\n",
    "\n",
    "        if len(G.nodes()) == 0:\n",
    "            logger.warning(\"No valid nodes found for visualization\")\n",
    "            return False\n",
    "\n",
    "        # Create visualization\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        pos = nx.spring_layout(G, k=2, iterations=50)\n",
    "        \n",
    "        # Draw nodes\n",
    "        nx.draw_networkx_nodes(G, pos, node_color='lightblue', \n",
    "                              node_size=3000, alpha=0.7)\n",
    "        \n",
    "        # Draw edges with varying thickness based on confidence\n",
    "        edges = G.edges()\n",
    "        if edges:\n",
    "            weights = [G[u][v].get('weight', 1) for u, v in edges]\n",
    "            nx.draw_networkx_edges(G, pos, edge_color='gray', \n",
    "                                 width=[w/2 for w in weights], alpha=0.6)\n",
    "        \n",
    "        # Draw labels\n",
    "        nx.draw_networkx_labels(G, pos, labels, font_size=8, font_weight='bold')\n",
    "        \n",
    "        # Draw edge labels\n",
    "        if edges:\n",
    "            edge_labels = nx.get_edge_attributes(G, 'label')\n",
    "            edge_weights = nx.get_edge_attributes(G, 'weight')\n",
    "            edge_text = {k: f\"{edge_labels.get(k, 'RELATED')} ({edge_weights.get(k, 0):.1f})\" \n",
    "                        for k in edge_labels}\n",
    "            nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_text, \n",
    "                                       font_size=6, font_color='red')\n",
    "        \n",
    "        plt.title(\"Entity-Relation Graph with Confidence Scores\", fontsize=16)\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if output_file:\n",
    "            plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "            logger.info(f\"Graph saved to {output_file}\")\n",
    "        \n",
    "        # plt.show()\n",
    "        plt.close()  # Prevent auto-display in Jupyter\n",
    "\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating visualization: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9098c9c",
   "metadata": {},
   "source": [
    "# CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "388c8eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json_response(response: str) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"Parse JSON response with better error handling.\"\"\"\n",
    "    if not response:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Try to find JSON in the response\n",
    "        response = response.strip()\n",
    "        if response.startswith('```json'):\n",
    "            response = response[7:]\n",
    "        if response.endswith('```'):\n",
    "            response = response[:-3]\n",
    "        response = response.strip()\n",
    "        \n",
    "        return json.loads(response)\n",
    "    except json.JSONDecodeError as e:\n",
    "        logger.error(f\"Failed to parse JSON response: {e}\")\n",
    "        logger.debug(f\"Response content: {response[:500]}...\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8bc5880",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_entities_and_links(folder: str, prompt_template: str, db_connector: SQLiteGraphConnector = None) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Process folder with improved error handling and database storage.\"\"\"\n",
    "    results = []\n",
    "    data_path = f\"./data/{folder}\"\n",
    "    \n",
    "    if not os.path.exists(data_path):\n",
    "        logger.error(f\"Data folder does not exist: {data_path}\")\n",
    "        return results\n",
    "    \n",
    "    files = glob.glob(f\"{data_path}/*\")\n",
    "    if not files:\n",
    "        logger.warning(f\"No files found in {data_path}\")\n",
    "        return results\n",
    "    \n",
    "    logger.info(f\"Processing {len(files)} files in {folder}\")\n",
    "    system_msg = \"You are an expert in extracting structured data from unstructured text documents. Return valid JSON only.\"\n",
    "    \n",
    "    for file_path in files:\n",
    "        try:\n",
    "            logger.info(f\"Processing file: {file_path}\")\n",
    "            \n",
    "            with open(file_path, \"r\", encoding='utf-8') as f:\n",
    "                content = f.read().strip()\n",
    "                \n",
    "            if not content:\n",
    "                logger.warning(f\"Empty file: {file_path}\")\n",
    "                continue\n",
    "                \n",
    "            prompt = Template(prompt_template).substitute(ctext=content)\n",
    "            response = run_llm_query(prompt, system_msg)\n",
    "            \n",
    "            if not response:\n",
    "                logger.error(f\"No response from LLM for file: {file_path}\")\n",
    "                continue\n",
    "                \n",
    "            parsed = parse_json_response(response)\n",
    "            if parsed and validate_graph_data(parsed):\n",
    "                results.append(parsed)\n",
    "                \n",
    "                # Store in graph database if connector provided\n",
    "                if db_connector:\n",
    "                    if store_in_graph_database(parsed, db_connector):\n",
    "                        logger.info(f\"Successfully stored data from {file_path} in graph database\")\n",
    "                    else:\n",
    "                        logger.error(f\"Failed to store data from {file_path} in graph database\")\n",
    "                \n",
    "                # Create output filename for graph\n",
    "                base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "                output_file = f\"./output/graph_{folder}_{base_name}.png\"\n",
    "                os.makedirs('./output', exist_ok=True)\n",
    "                visualize_graph(parsed, output_file)\n",
    "            else:\n",
    "                logger.error(f\"Invalid data structure in file: {file_path}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {file_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    logger.info(f\"Successfully processed {len(results)} files from {folder}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d8898ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_ingestion_process(folders: Dict[str, str], db_connector: SQLiteGraphConnector = None) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Main processing function with database integration.\"\"\"\n",
    "    all_results = []\n",
    "    \n",
    "    for folder, prompt in folders.items():\n",
    "        logger.info(f\"Starting processing for folder: {folder}\")\n",
    "        folder_results = parse_entities_and_links(folder, prompt, db_connector)\n",
    "        all_results.extend(folder_results)\n",
    "        logger.info(f\"Completed processing for folder: {folder}. Found {len(folder_results)} valid results.\")\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3b48c1",
   "metadata": {},
   "source": [
    "# PROMPT TEMPLATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cfdd6ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved prompt templates with clearer instructions\n",
    "project_prompt_template = \"\"\"\n",
    "Analyze the following Project Brief and extract entities and relationships. Return ONLY valid JSON in this exact format:\n",
    "\n",
    "{\n",
    "  \"entities\": [\n",
    "    {\"id\": \"unique_id\", \"label\": \"Project\", \"name\": \"project_name\", \"summary\": \"brief_summary\"},\n",
    "    {\"id\": \"unique_id\", \"label\": \"Technology\", \"name\": \"tech_name\"},\n",
    "    {\"id\": \"unique_id\", \"label\": \"Client\", \"name\": \"client_name\", \"industry\": \"industry_name\"},\n",
    "    {\"id\": \"unique_id\", \"label\": \"Risk\", \"description\": \"risk_description\"}\n",
    "  ],\n",
    "  \"relationships\": [\n",
    "    \"project_id|USES_TECH|technology_id\",\n",
    "    \"project_id|HAS_CLIENT|client_id\",\n",
    "    \"project_id|HAS_RISK|risk_id\"\n",
    "  ]\n",
    "}\n",
    "\n",
    "Project Brief:\n",
    "$ctext\n",
    "\"\"\"\n",
    "\n",
    "people_prompt_template = \"\"\"\n",
    "Extract information from the text and return ONLY valid JSON in this exact format:\n",
    "\n",
    "{\n",
    "  \"entities\": [\n",
    "    {\"id\": \"unique_id\", \"label\": \"Person\", \"name\": \"person_name\"},\n",
    "    {\"id\": \"unique_id\", \"label\": \"Project\", \"name\": \"project_name\", \"summary\": \"brief_summary\"},\n",
    "    {\"id\": \"unique_id\", \"label\": \"Technology\", \"name\": \"tech_name\"}\n",
    "  ],\n",
    "  \"relationships\": [\n",
    "    \"person_id|HAS_SKILLS|technology_id\",\n",
    "    \"project_id|HAS_PEOPLE|person_id\",\n",
    "    \"person_id|HAS_ROLE|project_id\"\n",
    "  ]\n",
    "}\n",
    "\n",
    "People Profiles:\n",
    "$ctext\n",
    "\"\"\"\n",
    "\n",
    "slack_prompt_template = \"\"\"\n",
    "Extract information from Slack messages and return ONLY valid JSON in this exact format:\n",
    "\n",
    "{\n",
    "  \"entities\": [\n",
    "    {\"id\": \"unique_id\", \"label\": \"Person\", \"name\": \"person_name\"},\n",
    "    {\"id\": \"unique_id\", \"label\": \"SlackMessage\", \"text\": \"message_text\"},\n",
    "    {\"id\": \"unique_id\", \"label\": \"Channel\", \"name\": \"channel_name\"}\n",
    "  ],\n",
    "  \"relationships\": [\n",
    "    \"person_id|SENT|message_id\",\n",
    "    \"message_id|POSTED_IN|channel_id\",\n",
    "    \"message_id|MENTIONS|person_id\"\n",
    "  ]\n",
    "}\n",
    "\n",
    "Slack Messages:\n",
    "$ctext\n",
    "\"\"\"\n",
    "\n",
    "# Configuration\n",
    "folders = {\n",
    "    \"people_profiles\": people_prompt_template,\n",
    "    \"project_briefs\": project_prompt_template,\n",
    "    \"slack_messages\": slack_prompt_template,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a0401d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 23:59:54,829 - INFO - Starting graph ingestion process with SQLite database\n",
      "2025-06-04 23:59:54,857 - INFO - Connected to SQLite database: graph_data.db\n",
      "2025-06-04 23:59:54,858 - INFO - Starting processing for folder: people_profiles\n",
      "2025-06-04 23:59:54,859 - INFO - Processing 2 files in people_profiles\n",
      "2025-06-04 23:59:54,860 - INFO - Processing file: ./data/people_profiles\\ex1.txt\n",
      "2025-06-04 23:59:55,898 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-04 23:59:58,124 - INFO - Successfully stored data from ./data/people_profiles\\ex1.txt in graph database\n",
      "2025-06-04 23:59:59,731 - INFO - Graph saved to ./output/graph_people_profiles_ex1.png\n",
      "2025-06-04 23:59:59,732 - INFO - Processing file: ./data/people_profiles\\ex2.txt\n",
      "2025-06-05 00:00:00,538 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 00:00:01,555 - ERROR - Invalid entity structure: {'id': 'AWSSageMaker', 'name': 'AWS SageMaker'}\n",
      "2025-06-05 00:00:01,556 - ERROR - Invalid data structure in file: ./data/people_profiles\\ex2.txt\n",
      "2025-06-05 00:00:01,557 - INFO - Successfully processed 1 files from people_profiles\n",
      "2025-06-05 00:00:01,558 - INFO - Completed processing for folder: people_profiles. Found 1 valid results.\n",
      "2025-06-05 00:00:01,559 - INFO - Starting processing for folder: project_briefs\n",
      "2025-06-05 00:00:01,561 - INFO - Processing 2 files in project_briefs\n",
      "2025-06-05 00:00:01,562 - INFO - Processing file: ./data/project_briefs\\ex1.txt\n",
      "2025-06-05 00:00:02,387 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 00:00:04,298 - INFO - Successfully stored data from ./data/project_briefs\\ex1.txt in graph database\n",
      "2025-06-05 00:00:05,788 - INFO - Graph saved to ./output/graph_project_briefs_ex1.png\n",
      "2025-06-05 00:00:05,789 - INFO - Processing file: ./data/project_briefs\\ex2.txt\n",
      "2025-06-05 00:00:06,582 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 00:00:08,539 - INFO - Successfully stored data from ./data/project_briefs\\ex2.txt in graph database\n",
      "2025-06-05 00:00:09,974 - INFO - Graph saved to ./output/graph_project_briefs_ex2.png\n",
      "2025-06-05 00:00:09,975 - INFO - Successfully processed 2 files from project_briefs\n",
      "2025-06-05 00:00:09,976 - INFO - Completed processing for folder: project_briefs. Found 2 valid results.\n",
      "2025-06-05 00:00:09,976 - INFO - Starting processing for folder: slack_messages\n",
      "2025-06-05 00:00:09,977 - INFO - Processing 2 files in slack_messages\n",
      "2025-06-05 00:00:09,977 - INFO - Processing file: ./data/slack_messages\\ex1.txt\n",
      "2025-06-05 00:00:10,882 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 00:00:11,886 - ERROR - Failed to parse JSON response: Invalid control character at: line 13 column 33 (char 634)\n",
      "2025-06-05 00:00:11,887 - ERROR - Invalid data structure in file: ./data/slack_messages\\ex1.txt\n",
      "2025-06-05 00:00:11,887 - INFO - Processing file: ./data/slack_messages\\ex2.txt\n",
      "2025-06-05 00:00:12,713 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 00:00:13,723 - ERROR - Failed to parse JSON response: Expecting ',' delimiter: line 15 column 2 (char 555)\n",
      "2025-06-05 00:00:13,724 - ERROR - Invalid data structure in file: ./data/slack_messages\\ex2.txt\n",
      "2025-06-05 00:00:13,724 - INFO - Successfully processed 0 files from slack_messages\n",
      "2025-06-05 00:00:13,725 - INFO - Completed processing for folder: slack_messages. Found 0 valid results.\n",
      "2025-06-05 00:00:13,725 - INFO - Process completed. Total results: 3\n",
      "2025-06-05 00:00:13,727 - INFO - Combined results saved to ./output/combined_results.json\n",
      "2025-06-05 00:00:13,727 - INFO - Running analytics demo...\n",
      "2025-06-05 00:00:13,728 - INFO - Entity counts: {'Technology': 5, 'Project': 2, 'Client': 1, 'Person': 1, 'Risk': 1}\n",
      "2025-06-05 00:00:13,729 - INFO - Relationship counts: {'HAS_SKILLS': 3, 'USES_TECH': 2, 'HAS_CLIENT': 1, 'HAS_PEOPLE': 1, 'HAS_RISK': 1, 'HAS_ROLE': 1}\n",
      "2025-06-05 00:00:13,730 - INFO - Most connected entities: [{'id': 'john_doe', 'label': 'Person', 'name': 'John Doe', 'connections': 5}, {'id': '1', 'label': 'Project', 'name': 'Recommendation Engine', 'connections': 4}, {'id': 'visionai', 'label': 'Project', 'name': 'VisionAI', 'connections': 2}, {'id': 'python', 'label': 'Technology', 'name': 'Python', 'connections': 1}, {'id': 'tensorflow', 'label': 'Technology', 'name': 'TensorFlow', 'connections': 1}]\n",
      "2025-06-05 00:00:13,731 - INFO - Projects found: 2\n",
      "2025-06-05 00:00:13,731 - INFO - Sample relationships: [{'source': 'john_doe', 'target': 'python', 'type': 'HAS_SKILLS', 'properties': '{\"confidence\": 4.988108575344086}', 'source_label': 'Person', 'source_name': 'John Doe', 'target_label': 'Technology', 'target_name': 'Python'}, {'source': 'john_doe', 'target': 'tensorflow', 'type': 'HAS_SKILLS', 'properties': '{\"confidence\": 4.986342191696167}', 'source_label': 'Person', 'source_name': 'John Doe', 'target_label': 'Technology', 'target_name': 'TensorFlow'}, {'source': 'john_doe', 'target': 'pytorch', 'type': 'HAS_SKILLS', 'properties': '{\"confidence\": 4.985095858573914}', 'source_label': 'Person', 'source_name': 'John Doe', 'target_label': 'Technology', 'target_name': 'PyTorch'}, {'source': 'visionai', 'target': 'john_doe', 'type': 'HAS_PEOPLE', 'properties': '{\"confidence\": 4.983096122741699}', 'source_label': 'Project', 'source_name': 'VisionAI', 'target_label': 'Person', 'target_name': 'John Doe'}, {'source': 'john_doe', 'target': 'visionai', 'type': 'HAS_ROLE', 'properties': '{\"confidence\": 4.978436231613159}', 'source_label': 'Person', 'source_name': 'John Doe', 'target_label': 'Project', 'target_name': 'VisionAI'}]\n",
      "2025-06-05 00:00:13,732 - INFO - Running SQLite-specific example queries...\n",
      "2025-06-05 00:00:13,733 - INFO - Project-Technology relationships: [{'id': '1', 'project_name': 'Recommendation Engine', 'tech_id': '2', 'tech_name': 'Apache Spark'}, {'id': '1', 'project_name': 'Recommendation Engine', 'tech_id': '3', 'tech_name': 'LightFM'}]\n",
      "2025-06-05 00:00:13,733 - INFO - SQLite connection closed\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        logger.info(\"Starting graph ingestion process with SQLite database\")\n",
    "        \n",
    "        # Initialize SQLite connector\n",
    "        db_connector = SQLiteGraphConnector(\"graph_data.db\")\n",
    "        \n",
    "        # Connect to database\n",
    "        if not db_connector.connect():\n",
    "            logger.error(\"Failed to connect to SQLite database. Exiting.\")\n",
    "            exit(1)\n",
    "        \n",
    "        # Process files and store in database\n",
    "        entities_relationships = graph_ingestion_process(folders, db_connector)\n",
    "        logger.info(f\"Process completed. Total results: {len(entities_relationships)}\")\n",
    "        \n",
    "        # Save combined results\n",
    "        output_path = \"./output/combined_results.json\"\n",
    "        os.makedirs('./output', exist_ok=True)\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(entities_relationships, f, indent=2, ensure_ascii=False)\n",
    "        logger.info(f\"Combined results saved to {output_path}\")\n",
    "        \n",
    "        # Run analytics\n",
    "        analytics = GraphAnalytics(db_connector)\n",
    "        run_analytics_demo(analytics)\n",
    "        \n",
    "        # Example queries specific to SQLite\n",
    "        logger.info(\"Running SQLite-specific example queries...\")\n",
    "        \n",
    "        # Find projects and their technologies\n",
    "        cursor = db_connector.connection.cursor()\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT e1.id, JSON_EXTRACT(e1.properties, '$.name') as project_name,\n",
    "                   e2.id as tech_id, JSON_EXTRACT(e2.properties, '$.name') as tech_name\n",
    "            FROM entities e1\n",
    "            JOIN relationships r ON e1.id = r.source\n",
    "            JOIN entities e2 ON r.target = e2.id\n",
    "            WHERE e1.label = 'Project' AND e2.label = 'Technology' AND r.type = 'USES_TECH'\n",
    "            LIMIT 10\n",
    "        \"\"\")\n",
    "        project_tech = [dict(row) for row in cursor.fetchall()]\n",
    "        logger.info(f\"Project-Technology relationships: {project_tech}\")\n",
    "        \n",
    "        # Close database connection\n",
    "        db_connector.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fatal error in main execution: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15b6f10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "K",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
